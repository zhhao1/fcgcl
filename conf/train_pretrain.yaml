# network architecture
# encoder related
elayers-speech: 6
elayers-text-src: 6
elayers-text-tgt: 0
elayers-shared: 0
eunits: 2048
# decoder related
dlayers: 6
dunits: 2048
# attention related
adim: 256
aheads: 4

#log name
name: MUST-C/pretrain_new

# label smoothing
lsm-weight: 0.1

#preprocess_conf
preprocess-conf:

#ctc shrink
ctc: false
ctc-shrink: false  #default ctc will be used when shrink is true

#contrastive_loss
fine-grand: false
share-attention: true
moco: false  # if true, remember to set max train epoch
num-negatives: 500 # number negatives in queue 
pretrain: true  #if true, the criterion need to be set with 'loss' 
cross-modal: true
cl-loss-scale: 1.0
temperature: 0.1
estimator: simclr
remove-pad: false #remove pad when averaging the encoder output

#projection head
mlp-layers: 3 #hidden layer: 3-2
output-dim: 128 #mlp output dim
hidden-dim: 512 #mlp hidden dim
share: true #whether share projection head or not
projection-type: non_linear #linear or non_linear
norm: false #layernorm the mlp hidden layer
mlp-droprate: 0.0 

#online kd
online-KD: false
online-KD-type: CE #CE or KL

#reconstruct speech input
reconstruct: false

#dropdim
dropdim-speech-encoder: false
dropdim-text-src-encoder: false
dropdim-text-tgt-encoder: false
dropdim-share-encoder: false
dropdim-decoder: false
mask-random: false
mask-span: false  #mask col with mean values
mask-prob: 0.05
mask-value: mean
mask-position: col
K: 30
block-number: 1

# minibatch related
batch-size: 90
maxlen-in: 512  # if input length  > maxlen-in, batchsize is automatically reduced
maxlen-out: 150 # if output length > maxlen-out, batchsize is automatically reduced

# optimization related
criterion: loss #acc or loss
sortagrad: 0 # Feed samples from shortest to longest ; -1: enabled for all epochs, 0: disabled
opt: noam
accumulate-grad-batches: 1
gradient-clip-val: 5
patience: 3
max-epochs: 50
dropout-rate: 0.1

# transformer specific setting
transformer-input-layer: linear     # encoder input architecture type
transformer-scale: 5.0
transformer-warmup-steps: 25000
transformer-attn-dropout-rate: 0.0
transformer-length-normalized-loss: false
transformer-init: pytorch

# pre-training related
enc-model-prefix: model
dec-model-prefix: model
text-src-model-prefix: model
#text-src-init: ../mt/logs/MUST-C/en-de/version_2/checkpoints/average_top_5_acc.ckpt
#enc-init: ../asr/logs/MUST-C/en-de/version_3/checkpoints/average_top_5_acc.ckpt
#dec-init: ./logs/MUST-C/en-de/version_26/checkpoints/average_top_5_acc.ckpt
enc-init-mods: encoder.embed,encoder.encoders,encoder.after_norm
dec-init-mods: decoder.embed,decoder.decoders,decoder.after_norm,decoder.output_layer
